# Q-Learning 高爾夫

中文版 | [English](README.md)

使用 Q-Learning 訓練智能體打迷你高爾夫的強化學習專案。

## 專案簡介

本專案實作了一個 Q-Learning 智能體，學習如何將高爾夫球打進洞裡。智能體會探索不同的角度和力道，根據洞口位置找到最佳擊球策略。

## 環境需求
```bash
pip install pygame numpy matplotlib
```

## 使用方法

執行訓練與展示：
```bash
python main.py
```

程式會依序執行：
1. 訓練智能體 1200 萬回合
2. 顯示學習曲線圖表
3. 進入展示模式，觀看訓練好的智能體表現

按 `Q` 鍵退出展示模式。

## 專案結構

- `main.py` - 訓練迴圈與展示模式
- `src/agent.py` - Q-Learning 智能體實作
- `src/env.py` - 高爾夫環境與物理模擬
- `src/config.py` - 遊戲參數與超參數設定

## Q-Learning 演算法

智能體使用 Q-Learning 更新規則：

$$Q(s, a) \leftarrow Q(s, a) + \alpha [r - Q(s, a)]$$

其中：
- $Q(s, a)$ 是狀態 $s$ 與動作 $a$ 的 Q 值
- $\alpha$ 是學習率 (0.17)
- $r$ 是即時獎勵

由於這是回合制任務（每回合一次擊球），目標值簡化為獎勵 $r$，不考慮未來狀態。

## 獎勵函數

$$\text{reward} = \max(0, 100 - \frac{d}{3}) + \begin{cases} 
1000 & \text{球進洞} \\
0 & \text{否則}
\end{cases}$$

其中 $d$ 是球與洞口之間的歐幾里得距離。

## 狀態與動作空間

- **狀態空間**：基於網格的洞口位置離散化（66 × 100 格）
- **動作空間**：663 個動作（51 個角度 × 13 個力道）
  - 角度：-45° 至 45°，共 51 階
  - 力道：0.2 至 1.0，共 13 階

## 超參數設定

| 參數 | 數值 |
|------|------|
| 學習率 (α) | 0.17 |
| 折扣因子 (γ) | 0.95 |
| 初始探索率 (ε) | 1.0 |
| 最小探索率 | 0.01 |
| 探索率衰減 | 0.9999997 |

## 訓練結果

訓練完成後，智能體能夠穩定地將球打到接近或進入洞口，並根據洞口位置調整擊球策略。

## 授權

MIT